{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from random import random\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "counter = 0\n",
    "control = True \n",
    "def tiki_scraper(category_url):\n",
    "    page = 205\n",
    "    while True:\n",
    "        print(page)\n",
    "        \n",
    "        # generates random delay between 0.5 and 1.5 seconds:\n",
    "        delay = random() + 0.5\n",
    "\n",
    "        # defines target page url \n",
    "        current = category_url + str(page)\n",
    "\n",
    "        # general url business and creating soup obj:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}\n",
    "\n",
    "        r = requests.get(current, headers=headers)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        # populates list of all products on target page:\n",
    "        products = soup.find_all(\"a\", {\"class\": \"product-item\"}) \n",
    "\n",
    "        while len(products) == 0:\n",
    "            r = requests.get(current, headers=headers)\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "            products = soup.find_all(\"a\", {\"class\": \"product-item\"})\n",
    "            counter += 1\n",
    "            if counter == 20:\n",
    "                control = False \n",
    "                break\n",
    "\n",
    "        \n",
    "        if not control:\n",
    "            break \n",
    "        print(f\"Currently scraping {len(products)} products from page {page}.\")\n",
    "      \n",
    "        for product in products:\n",
    "            #creates dictionary of product info:\n",
    "            d = {}\n",
    "            d['product_id'] = (product['href'].split('-')[-1])[:-5]    \n",
    "            d['title'] = product.find('div', {'class':'name'}).text\n",
    "            d['price'] = product.find('div', {'class':'price-discount__price'}).text\n",
    "            d['image_url'] = product.img['src']\n",
    "            d['free_shipping'] = bool(product.find('div', {'class':'badge-top'}))\n",
    "            d['reviewer_tot'] = (product.find('div', {'class':'review'}).text).strip('()')\n",
    "            data.append(d)\n",
    "        page += 1\n",
    "\n",
    "        #delays next loop initiation\n",
    "        time.sleep(delay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiki_scraper('https://tiki.vn/laptop-may-vi-tinh/c1846?src=c.1846.hamburger_menu_fly_out_banner&page=')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(data)\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}